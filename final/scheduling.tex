\vspace{-0.25em}
\section{GPU Scheduling}
\label{sec:scheduling}
\vspace{-0.25em}

The goal of the Gdev scheduler is to correctly assign computation and
data transmission times for each GPU context based on the given
scheduling policy.
Although we make use of some previous
techniques~\cite{Kato_RTSS11,Kato_ATC11}, Gdev provides a new queuing
scheme and virtual GPU support for multi-tasking systems.
Gdev also propagates the task priority used in the OS to the GPU context.

\vspace{-0.25em}
\subsection{Scheduling and Queuing}
\label{sec:scheduling_queueing}
\vspace{-0.25em}

Gdev uses a similar scheme to TimeGraph~\cite{Kato_ATC11} for GPU
scheduling.
Specifically, it allows GPU contexts to use GPU resources only when no
other contexts are using the corresponding resources.
The stalling GPU contexts are queued by the Gdev scheduler while waiting for
the current context to leave the resources.
In order to notify the completion of the current context execution, Gdev
uses additional GPU commands to generate an interrupt from the GPU.
Upon every interrupt, the highest-priority context is dispatched to the
GPU from the waiting queue.
Computation and data transmission times are separately accumulated for
resource accounting.
For computations, we allow the same context to launch multiple
compute instances simultaneously, and the total makespan from the first
to the last instance is deemed as the computation time.
PTask~\cite{Rossbach_SOSP11} and RGEM~\cite{Kato_RTSS11} also provide
similar schedulers, but do not use interrupts, and hence resource accounting is
managed by the user space via the API.

Gdev is API-driven where the scheduler is invoked only when computation or
data transmission requests are submitted, whereas TimeGraph is
command-driven, which invokes the scheduler whenever GPU commands are
flushed.
In this regard, Gdev is similar to PTask~\cite{Rossbach_SOSP11} and
RGEM~\cite{Kato_RTSS11}.
However, Gdev differs from these two approaches in that it can separate
queues for accounting of computations and data transmissions, which we
call the \textit{Multiple Resource Queues} (MRQ) scheme.
On the other hand, what we call the \textit{Single Device Queue} (SDQ)
scheme uses a single queue per device for accounting.

The MRQ scheme is apparently more efficient than the SDQ scheme, when
computations and data transmissions can be overlapped.
Suppose that there are two contexts both requesting 50\% of computation
and 50\% of data transmission demands.
The SDQ scheme presumes that the demand of each context is 50 + 50 =
100\%, implying a total demand of 200\% by the two contexts.
As a result, this workload looks overloaded under the SDQ scheme.
The MRQ scheme, on the other hand, does not consider the total workload
to be overloaded due to overlapping but each resource to be fully utilized.

Gdev creates two different scheduler threads to control the resource
usage of the GPU compute unit and DMA unit separately.
The compute scheduler thread is invoked by GPU interrupts generated upon
the completion of each GPU compute operation, while the DMA scheduler
thread is awakened by the Gdev runtime system when the memory-copy operation is
completed, since we do not use interrupts for memory-copy operations.

\subsection{GPU Virtualization}
\label{sec:virtual_gpu}
\vspace{-0.25em}

Gdev is able to virtualize a physical GPU into multiple logical GPUs to
protect working groups of multi-tasking systems from interference.
Virtual GPUs are activated by specifying weights of GPU resources
assigned to each of them.
GPU resources are classified to \textit{memory share}, \textit{memory
bandwidth}, and \textit{compute bandwidth}.
Memory share is the weight of physical memory available for the
virtual GPU.
Memory bandwidth is the amount of time in a certain period allocated for
memory-copy operations using the virtual GPU, while compute
bandwidth is that for compute operations.
Regarding memory share, Gdev simply partitions physical memory space.
Meanwhile, we provide the GPU scheduler to meet the requirements
of compute and memory-copy bandwidth.
Considering similar characteristics of non-preemptive computations and
data transmissions, we apply the same policy to the compute and
memory-copy schedulers.

The challenge for virtual GPU scheduling is raised by the non-preemptive
and burst nature of GPU workloads.
We have implemented the Credit scheduling algorithm supported by Xen
hypervisor~\cite{Barham_SOSP03} to verify if an existing virtual CPU
scheduling policy can be applied for a virtual GPU scheduler.
However, we have found that the Credit scheduler fails to maintain the
desired bandwidth for the virtual GPU, largely attributed to the fact
that it presumes preemptive constantly-working CPU workloads, while GPU
workloads are non-preemptive and bursting.

\begin{figure}[t]
 \begin{center}
  \begin{tabular}{l}
   \hline
   {\small \verb|vgpu->bgt|: budget of the virtual GPU.}\\
   {\small \verb|vgpu->utl|: actual GPU utilization of the virtual GPU.}\\
   {\small \verb|vgpu->bw|: bandwidth assigned to the virtual
   GPU.}\\
   {\small \verb|current/next|: current/next virtual GPU selected for run.}\\
   \hline
   {\small \verb|void on_arrival(vgpu, ctx) {|}\\
   {\small \verb|  if (current && current != vgpu)|}\\
   {\small \verb|    suspend(ctx);|}\\
   {\small \verb|  dispatch(ctx);|}\\
   {\small \verb|}|}\\
   {\small \verb|VirtualGPU on_completion(vgpu, ctx) {|}\\
   {\small \verb|  if (vgpu->bgt < 0 && vgpu->utl > vgpu->bw)|}\\
   {\small \verb|    move_to_queue_tail(vgpu);|}\\
   {\small \verb|  next = get_queue_head();|}\\
   {\small \verb|  if (!next) return null;|}\\
   {\small \verb|  if (next != vgpu && next->utl > next->bw) {|}\\
   {\small \verb|    wait_for_short();|}\\
   {\small \verb|    if (current) return null;|}\\
   {\small \verb|  }|}\\
   {\small \verb|  return next;|}\\
   {\small \verb|}|}\\
   \hline
  \end{tabular}
  \caption{Pseudo-code of the BAND scheduler.}
  \label{fig:band}
 \end{center}
 \vspace{-1.7em}
\end{figure}

To overcome the virtual GPU scheduling problem, we propose a
\textit{bandwidth-aware non-preemptive device} (BAND) scheduling
algorithm.
The pseudo-code of the BAND scheduler is shown in
Figure~\ref{fig:band}.
The \texttt{on\_arrival} function is called when a GPU context
(\texttt{ctx}) running on a virtual GPU (\texttt{vgpu}) attempts to
access GPU resources for computations or data transmissions.
The context can be dispatched to the GPU only if no other virtual GPUs
are using the GPU.
Otherwise, the corresponding task is suspended.
The \texttt{on\_completion} function is called by the scheduler thread
upon the completion of the context (\texttt{ctx}) assigned to the
virtual GPU (\texttt{vgpu}), selecting the next virtual GPU to operate.

The BAND scheduler is based on the Credit scheduler, but differs in the
following two points.
First, the BAND scheduler lowers the priority of the virtual GPU, when
its budget (credit) is exhausted \textit{and} its actual utilization of
the GPU is exceeding the assigned bandwidth, whereas the Credit
scheduler always lowers the priority, when the budget is exhausted.
This prioritization compensates for credit errors posed due to
non-preemptive executions.

The second modification to the Credit scheduler is that the BAND
scheduler waits for a certain amount of time specified by the system
designer, if the GPU utilization of the virtual GPU selected
by the scheduler is exceeding its assigned bandwidth.
This ``time-buffering'' approach works for non-preemptive burst workloads.
Suppose that the system has two virtual GPUs, both of which run some
burst-workload GPU contexts, but their non-preemptive execution times
are different.
If the contexts arrive in turn, they are also dispatched to the GPU in
turn, but the GPU utilization could not be fair due to different lengths
of non-preemptive executions.
If the scheduler waits for a short interval, however, the context with a
short length of non-preemptive execution could arrive with the next
request, and the \texttt{on\_arrival} function can dispatch it to the
GPU while the scheduler is waiting.
Thus, resource allocations could become fairer.
In this case, we need not to select the next virtual GPU, since the
\texttt{on\_arrival} function has already dispatched one.
If no contexts have arrived, however, we return the selected virtual GPU.
This situation implies that there are no burst workloads, and hence no
emergency to meet the bandwidth.
