\vspace{-0.25em}
\section{Experimental Evaluation}
\label{sec:evaluation}
\vspace{-0.25em}

We evaluate our Gdev prototype, using the Rodinia
benchmarks~\cite{Che_IISWC09}, GPU-accelerated eCryptfs encrypted
file system from KGPU~\cite{Sun_SYSTOR12}, FAST database
search~\cite{Kim_SIGMOD10}, and some dataflow
microbenchmarks from PTask~\cite{Rossbach_SOSP11}.
We disclose that the basic performance of our prototype is practical
even compared to proprietary software, and also demonstrate that Gdev
provides significant benefits for GPU applications in time-sharing
systems.

Our experiments are conducted with the Linux kernel 2.6.39 on NVIDIA
GeForce~GTX~480 graphics card and Intel Core~2~Extreme QX9650 processor.
GPU programs are written in CUDA and compiled by NVCC~\cite{CUDA40},
while CPU programs are compiled by gcc 4.4.6.

\vspace{-0.25em}
\subsection{Basic Performance}
\vspace{-0.25em}

\begin{figure}[t]
 \begin{center}
  \includegraphics[width=0.8\hsize]{eps/chunk.eps}\\
  \vspace{-1.5em}
  \caption{Impact of the chunk size on DMA speeds.}
  \label{fig:chunk}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.8\hsize]{eps/dma.eps}\\
  \vspace{-1.5em}
  \caption{Relative speed of I/O access to DMA.}
  \label{fig:io_access}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/memcpy.eps}\\
  \vspace{-1.5em}
  \caption{Memory-copy throughput.}
  \label{fig:memcpy}
 \end{center}
 \vspace{-1.5em}
\end{figure}

We evaluate the standalone performance of applications achieved by
our Gdev prototype to argue that the rest of our evaluation is practical
in the real world.
To this end, first of all, we need to find the best parameters used for
memory-copy optimization, using simple test code that copies data
between device and host memory.

Figure~\ref{fig:chunk} shows the impact of the chunk size on data
transfer times for host-to-device (HtoD) and device-to-host (DtoH)
directions respectively, when using DMA-based memory-copy operations
with 256MB and 512MB of data.
Since each chunk incurs some overhead in DMA configuration, a
smaller chunk size producing a greater number of chunks increases a
transfer time.
On the other hand, there is a constraint that the first and last pieces
of chunks cannot be overlapped with others, as described in
Section~\ref{sec:memory_copy}.
Hence, a larger chunk size leading to a longer blocking time with these
pieces of chunks also increases a transfer time.
According to our observation, a chunk size of 4MB is the best trade-off
for both HtoD and DtoH directions.
We therefore set the chunk size to 4MB for our experiments.

Figure~\ref{fig:io_access} shows the relative speed of direct I/O
access to DMA for a small size of data. 
Due to some hardware effect, HtoD and DtoH directions show different
transfer times, but it clearly explains the advantage of direct I/O
access for small data.
According to our observation, the data transfer speed inverses around a
data size of 4KB and 1KB for HtoD and DtoH directions respectively.
We therefore set the boundary of direct I/O access and DMA to 4KB and
1KB for them respectively.

Figure~\ref{fig:memcpy} shows memory-copy throughput achieved
by our Gdev prototype compared to NVIDIA's proprietary software.
``Gdev/User'' employs a runtime library in the user-space, while
``Gdev'' integrates runtime support into the OS.
Interestingly, user-space runtime achieves higher throughput than
OS-space runtime, particularly for DtoH direction.
This difference comes from \texttt{memcpy}'s effect within host memory.
In fact, the \texttt{memcpy} implementation in the Linux kernel is slower
than that in the user-space GNU library, when copying data from host I/O
to main memory.
This could be a disadvantage of our approach.
We will investigate this effect more in depth.
Apart from the DtoH memory-copy throughput, however, our Gdev prototype
and NVIDIA's proprietary software are almost competitive.
 
\begin{table}[t]
 \caption{List of benchmarks.}
 \label{tab:benchmarks}
 \vspace{-0.5em}
 \begin{center}
  {\footnotesize
  \begin{tabular}{|l|l|}
   \hline
   \textbf{Benchmark} & \textbf{Description}\\
   \hline
   LOOP & Long-loop compute without data \\
   \hline
   MADD & 1024x1024 matrix addition\\
   \hline
   MMUL & 1024x1024 matrix multiplication\\
   \hline
   CPY & 256MB of HtoD and DtoH\\
   \hline
   PINCPY & CPY using pinned host I/O memory\\
   \hline
   BP & Back propagation (pattern recognition)\\
   \hline
   BFS & Breadth-first search (graph algorithm)\\
   \hline
   HW & Heart wall (medical imaging)\\
   \hline
   HS & Hotspot (physics simulation)\\
   \hline
   LUD & LU decomposition (linear algebra)\\
   \hline
   NN & K-nearest neighbors (data mining)\\
   \hline
   NW & Needleman-wunsch (bioinformatics)\\
   \hline
   SRAD & Speckle reducing anisotropic diffusion (imaging)\\
   \hline
   SRAD2 & SRAD with random pseudo-inputs (imaging)\\
   \hline
  \end{tabular}
  }
 \end{center}
 \vspace{-1.5em}
\end{table}

Figure~\ref{fig:basic_performance} demonstrates the standalone
performance of benchmarks achieved by our Gdev prototype compared to
NVIDIA's proprietary software.
Table~\ref{tab:benchmarks} describes the microbenchmarks and
Rodinia~\cite{Che_IISWC09} benchmarks used in this evaluation.
First of all, we have found that NVIDIA GPUs have some ``performance
mode'' to boost hardware performance that we do not use for our Gdev
prototype implementation.
As observed in the LOOP benchmark result, our Gdev prototype incurs
about 20\% of decrease in performance compared to the proprietary
software due to a lack of performance mode.
However, the impact of performance mode is workload dependent.
If workloads are very compute-intensive, such as the HW and SRAD
benchmarks, this impact appears clearly, whereas some friendly
workloads, such as the BFS and HS benchmarks, are not influenced very
much.
In either case, however, this is an implementation issue, but is
not a conceptual limitation of Gdev.
These benchmark results also imply that Gdev's runtime-unified OS
approach would not be appreciated by data-intensive workloads.
For example, the BP benchmark is associated with a very large size of data,
though its compute demand is not very high.
This type of workload would not perform well with our Gdev prototype, since
the \texttt{memcpy} function of the Linux kernel becomes a bottleneck.
On the other hand, the PINCPY benchmark does not need \texttt{memcpy}
operations.
Hence, performance does not depend on implementations.

\begin{figure}[t]
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/basic_performance.eps}\\
  \vspace{-1.5em}
  \caption{Basic standalone performance.}
  \label{fig:basic_performance}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/scheduler_overhead.eps}\\
  \vspace{-1.5em}
  \caption{Unconstrained real-time performance.}
  \label{fig:scheduler_overhead}
 \end{center}
 \vspace{-1.5em}
\end{figure}

\vspace{-0.25em}
\subsection{Reliability}
\vspace{-0.25em}

We next evaluate reliability of runtime support integrated in the OS.
Figure~\ref{fig:scheduler_overhead} compares the performances of the
OS-space API-driven scheme (Gdev and PTask~\cite{Rossbach_SOSP11}),
the OS-space command-driven scheme (TimeGraph~\cite{Kato_ATC11} and
GERM~\cite{Bautin_MCNC08}), and the user-space API-driven scheme
(RGEM~\cite{Kato_RTSS11}).
We run Rodinia benchmarks recursively as fast as possible as
real-time tasks, contending with such background tasks that bypass the
user-space library and launch many meaningless GPU commands.
The user-space API-driven scheme severely suffers from this situation,
since it cannot schedule these bypassing tasks at all.
The OS-space command-driven scheme is able to sustain the interference
to some extent by using the GPU command scheduler, but the overhead is
non-trivial due to many scheduler invocations.
On the other hand, the OS-space API-driven scheme can reject such command
submission that is not submitted through the API.
Gdev and PTask are both API-driven, but PTask exposes the system call to
user-space programs, which could allow misbehaving tasks to abuse GPU
resources.
This evinces that Gdev's approach that integrates runtime support into
the OS is a more reliable solution.

\vspace{-0.25em}
\subsection{GPU Acceleration for the OS}
\vspace{-0.25em}

\begin{figure}[t]
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/ecryptfs_read.eps}\\
  \vspace{-1.5em}
  \caption{eCryptfs read throughput.}
  \label{fig:ecryptfs_read}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/ecryptfs_write.eps}\\
  \vspace{-1.5em}
  \caption{eCryptfs write throughput.}
  \label{fig:ecryptfs_write}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.9\hsize]{eps/ecryptfs_write_multitask.eps}\\
  \vspace{-1.5em}
  \caption{eCryptfs write throughput with priorities.}
  \label{fig:ecryptfs_write_multitask}
 \end{center}
 \vspace{-1.5em}
\end{figure}

We now evaluate the performance of the Linux encrypted file system
accelerated by the GPU.
In particular, we use KGPU's implementation of
eCryptfs~\cite{Sun_SYSTOR12}.
KGPU is a framework that allows the OS to access the user-space runtime
library to use GPUs for computations.
We have modified KGPU's eCryptfs implementation to call the CUDA API
functions provided by Gdev directly instead of sending requests to the
KGPU user-space daemon.

Figure~\ref{fig:ecryptfs_read} and \ref{fig:ecryptfs_write} show the
read and write throughput of several versions of eCryptfs.
``CPU'' represents the CPU implementation, while ``KGPU \& NVIDIA'' and
``KGPU \& Gdev/User'' represent those using KGPU with NVIDIA's library
and Gdev's library respectively in the user space. 
``Gdev'' is our contribution that enables the eCryptfs module to use the
GPU directly within the OS.
Due to some page cache effect, read and write are not identical in
throughput, but an advantage of using the GPU is clearly depicted.
One may observe that Gdev's runtime-unified OS approach does not really
outperform KGPU's approach.
This is not surprising at all, because a magnitude of improvements in
latency achieved by our OS approach would be at most microseconds, while
the AES/DES operations of eCryptfs performed on the GPU are
orders-of-milliseconds.
Nonetheless, Gdev provides a significant benefit that the OS is freed
from the user space, and thus is more secure.

A further advantage of using Gdev appears in a multi-tasking scenario.
Figure~\ref{fig:ecryptfs_write_multitask} shows the write throughput of
eCryptfs when the FAST search task~\cite{Kim_SIGMOD10} is competing for
the GPU.
Since Gdev supports priorities in the OS, we can assign the eCryptfs
task with the highest priority, while the search task is still assigned
a higher priority than other tasks.
Using KGPU in this scenario, however, the performance of eCryptfs is
affected by the search task due to a lack of prioritization, as observed
in ``KGPU \& NVIDIA''.
Even with priorities, KGPU could suffer from a priority inversion
problem, where the high-priority eCryptfs task is reduced to the KGPU
priority level when accessing the GPU, while the search task is
executing at the higher priority level.
We could assign a high priority to the user-space KGPU daemon to avoid
this priority inversion problem, but it affects all user-space GPU
applications performance. 
On the other hand, Gdev can assign each GPU application with an
identical priority, which addresses the priority inversion problem
fundamentally.

\vspace{-0.25em}
\subsection{Effect of Shared Device Memory}
\vspace{-0.25em}

\begin{figure}[t]
 \begin{center}
  \includegraphics[width=\hsize]{eps/dataflow.eps}\\
  \vspace{-1.5em}
  \caption{Impact of shared memory on dataflow tasks.}
  \label{fig:dataflow}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.75\hsize]{eps/swapping.eps}\\
  \vspace{-1.5em}
  \caption{Impact of swapping latency.}
  \label{fig:swapping}
 \end{center}
 \vspace{-1.5em}
 \begin{center}
  \includegraphics[width=0.75\hsize]{eps/swapping_vgpu.eps}\\
  \vspace{-1.5em}
  \caption{Impact of swapping latency on virtual GPUs.}
  \label{fig:swapping_vgpu}
 \end{center}
 \vspace{-1.5em}
\end{figure}

\begin{figure*}[t]
 \begin{center}
  \subfigure[FIFO scheduler] {
  \includegraphics[width=0.318\hsize]{eps/vgpu_2_fifo.eps}
  }
  \subfigure[Credit scheduler] {
  \includegraphics[width=0.318\hsize]{eps/vgpu_2_credit.eps}
  }
  \subfigure[BAND scheduler] {
  \includegraphics[width=0.318\hsize]{eps/vgpu_2_band.eps}
  }
  \vspace{-1.4em}
  \caption{Util. of virtual GPUs under unfair workloads.}
  \label{fig:vgpu_2}
  \end{center}
  \vspace{-1.5em}
\end{figure*}

Figure~\ref{fig:dataflow} shows the speedups of dataflow benchmarks
brought by Gdev's shared device memory functionality.
Respecting PTask's setup~\cite{Rossbach_SOSP11} for a similar
evaluation, we make a dataflow by a 6x32 tree or a 6x10 rectangle.
``NVIDIA/modular'' and ``Gdev/modular'' use NVIDIA's and Gdev's CUDA
implementations respectively, where a dataflow program is implemented
in such a way that allocates a self-contained context to each graph node
as a module, and connects its output and input by copying data between
host and device memory back and forth.
``Gdev/shm'' uses shared device memory,
\textit{i.e.}, it connects output and input by sharing the same ``key''
associated with the same memory space.
According to the results, shared device memory is fairly effective for
dataflows with large data.
For example, it gains a 49\% speedup for the 1024x1024 madd tree.
Specifically, ``Gdev/modular'' took 1424ms while ``Gdev/shm'' took 953ms
to complete this dataflow. 
This indeed makes sense.
The average data transfer time for a 1024x1024 integer value was about
8ms, and we can reduce data communications by a total of
32+16+8+4+2=62 intermediate nodes for a 6x32 tree, which results in a
total reduced time of 8x62=496ms.
It should be noted that PTask achieves more speedups due to advanced
dataflow scheduling~\cite{Rossbach_SOSP11}.
However, we provide users with a first-class API primitive to manage
shared device memory, which could be used as a generic IPC method to
address different problems.
Therefore, we distinguish our contribution from PTask.
In addition, it is surprising that our prototype system outperforms the
proprietary software significantly.
We suspect that the proprietary one takes a long time to initialize
contexts when there are many active contexts, though an in-depth
investigation is required.

Figure~\ref{fig:swapping} depicts the impact of memory swapping on the
makespan of multiple 128MB-data FAST search tasks, when another 1GB-data
FAST search task runs at the highest priority level. 
Given that the GPU used in this evaluation supports 1.6GB of device
memory, we cannot create more than three 128MB-data search tasks at once
if memory swapping is not provided.
Memory swapping uses shared device memory, which needs access to the
page table.
Hence, our prototype implementation does not support memory swapping as
well as shared device memory for ``Gdev/User'', and it fails when
the number of the small search tasks exceeds three.
It is interesting to observe that NVIDIA' proprietary software fails
when the number of the small search tasks exceeds one.
This is because NVIDIA's proprietary software reserves some
amount of device memory for other purposes.
Unlike the user-space runtime approaches, Gdev's runtime-unified OS
approach can support memory swapping, and all the 128MB-data search
tasks can survive under this memory pressure.
However, a reflection point where the slope of increase in the makespan
changes is different, depending on whether the temporal swap
space allocated on device memory is used or not.
When the temporal swap space is not used, the reflection point is
clearer as observed in ``Gdev w/o swp'', because the swapping latency is
not trivial due to data movement between host and device memory.
Using the temporal swap space, on the other hand, we can reduce the impact
of memory swapping on the makespan of the search tasks, but the
reflection point appears slightly earlier, since the temporal swap space
itself occupies certain space on device memory.

Figure~\ref{fig:swapping_vgpu} shows the impact of memory swapping on
virtual GPUs.
In this experiment, we introduce virtual GPUs, and execute 128MB-data
search tasks on the first virtual GPU.
The memory size available for the virtual GPU is more restricted in
the presence of more virtual GPUs.
We confirm that the makespans become longer and their reflection points
appear earlier for a greater number of virtual GPUs, but all the search
tasks can still complete.
This explains that memory swapping is also useful on virtual GPUs.

\vspace{-0.25em}
\subsection{Isolation among Virtual GPUs}
\vspace{-0.25em}

We now evaluate Gdev in terms of the isolation among virtual GPUs.
Figure~\ref{fig:vgpu_2} demonstrates the actual GPU utilization of two
virtual GPUs, achieved by the FIFO, Credit, and BAND schedulers under
the SDQ scheme.
VGPU~0 executes the LUD benchmark to produce short-length tasks, while
VGPU 1 executes the HW benchmark to produce long-length tasks.
These tasks run repeatedly for 200 seconds to impose high workloads on
the entire system.
To see a workload change clearly, VGPU~1 is started 30 seconds after
VGPU~0.
Our observation is that the FIFO scheduler is not capable of enforcing
isolation at all.
The Credit scheduler also fails to provide isolation, since it is not
designed to handle non-preemptive burst workload.
The BAND scheduler, however, can almost provide the desired GPU
utilization, thanks to the time-buffering policy that allows
short-length tasks to meet the assigned bandwidth.
An error in the utilization of two virtual GPUs is retained
within 7\% on average.

We next study the effectiveness of the MRQ scheme that separates the
queues for compute and memory-copy operations.
Figure~\ref{fig:vgpu_2_band_mrq} illustrates the utilization of two
virtual GPUs under the BAND scheduler, executing the SRAD benchmark
tasks with different sizes of image.
We noticed that the compute and memory-copy operations can be
overlapped, but they affect the run-to-completion time with each other.
When VGPU 1 uses more compute resources due to a large size of
computation, the length of memory-copy operations requested by VGPU 0 is
prolonged due to overlapping.
As a result, it requires more memory-copy bandwidth.
However, the available bandwidth is capped by the BAND scheduler,
\textit{i.e.}, both the compute and memory-copy operations are limited 
to about 50\% of bandwidth at most.
One can also observe that the MRQ scheme allowed the sum of compute and
memory-copy bandwidth to exceed 100\%.

We finally demonstrate the scalability of our virtual GPU support.
Figure~\ref{fig:vgpu_fair_4_band} shows the utilization of four virtual
GPUs under the BAND scheduler, where all virtual GPUs execute four
instances of the LUD benchmark task exhaustively to produce fair
workloads.
The workloads of each virtual GPU begin in turn at an interval of 30
seconds.
Under such sane workloads, our virtual GPU support can provide fair
bandwidth allocations, even if the system exhibits non-preemptive burst
workloads.

\begin{figure}[t]
 \begin{center}
  \includegraphics[width=0.93\hsize]{eps/vgpu_2_band_compute.eps}\\
  \vspace{-0.5em}
  \includegraphics[width=0.93\hsize]{eps/vgpu_2_band_memory.eps}\\
  \vspace{-1.5em}
  \caption{Util. of virtual GPUs with the MRQ scheme (upper for compute
  and lower for memory-copy).}
  \label{fig:vgpu_2_band_mrq}
 \end{center}
 \begin{center}
  \includegraphics[width=0.93\hsize]{eps/vgpu_fair_4_band.eps}\\
  \vspace{-1.5em}
  \caption{Util. of virtual GPUs under fair workloads.}
  \label{fig:vgpu_fair_4_band}
 \end{center}
  \vspace{-1.5em}
\end{figure}
