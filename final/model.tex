\vspace{-0.25em}
\section{System Model}
\label{sec:model}
\vspace{-0.25em}

This paper focuses on a system composed of a GPU and a multi-core CPU.
GPU applications use a set of the API supported by the system, typically
taking the following steps:
(i) allocate space to device memory, 
(ii) copy data to the allocated device memory space, 
(iii) launch the program on the GPU, 
(iv) copy resultant data back to host memory, and 
(v) free the allocated device memory space.
We also assume that the GPU is designed based on NVIDIA's \textit{Fermi}
architecture~\cite{Fermi}.
The concept of Gdev, however, is not limited to Fermi, but is also
applicable to those based on the following model.

\textbf{Command:}
The GPU operates using the architecture-specific commands.
Each GPU context is assigned with a FIFO queue to which the program
running on the CPU submits the commands.
Computations and data transfers on the GPU are triggered only when the
corresponding commands are dispatched by the GPU itself.

\textbf{Channel:}
Each GPU context is assigned with a GPU hardware channel within which
command dispatching is managed.
Fermi does not permit multiple channels to access the
same GPU functional unit simultaneously, but allow them to coexist being
switched automatically in hardware.
This constraint may however be removed in the future architectures or
product lines.

\textbf{Address Space:}
Each GPU context is assigned with virtual address space managed through
the page table configured by the device driver.
Address translations are performed by he memory management unit on the
GPU.

\begin{comment}
\textbf{I/O Register:}
The GPU provides a bunch of memory-mapped I/O registers per context
visible to the device driver through the (PCI) I/O bus.
The device driver needs to manage these registers to send commands and
set up channels and address space.
\end{comment}

\textbf{Compute Unit:}
The GPU maps \textit{threads} assigned by programmers to \textit{cores}
on the compute unit.
This thread assignment is not visible to the system, implying that GPU
resource management at the system level should be context-based. 
Multiple contexts cannot execute on the compute unit at once due to the
channel constraint, but multiple requests issued from the same context
can be processed simultaneously.
We also assume that GPU computation is non-preemptive.

\textbf{DMA Unit:}
There are two types of DMA units for data transmission: (i) synchronous
with the compute unit and (ii) asynchronous.
Only the latter type of DMA units can overlap their operations with the
compute unit.
We also assume that DMA transaction is non-preemptive.
