\vspace{-0.25em}
\section{Introduction}
\label{sec:introduction}
\vspace{-0.25em}

Recent innovations in heterogeneous compute devices with many-core
technology have achieved an order-of-magnitude gain in computing
performance. 
In particular, the graphics processing unit (GPU) receives considerable
attention as a mature heterogeneous compute device that embraces a
concept of many-core computing.
A recent announcement from the TOP500 supercomputing sites in November
2011~\cite{TOP500} disclosed that three of the top five supercomputers
use GPUs.
For example, scientific climate applications can gain 80x speedups using
such GPU-based supercomputers~\cite{Shimokawabe10}.
The benefit of GPUs appears not only in high-performance computing but
also in general-purpose and embedded computing domains.
According to previous research, GPUs can achieve up to an order of 10x
speedups for software routers~\cite{Han_SIGCOMM10}, 20x speedups for
encrypted networks~\cite{Jang_NSDI11}, and 15x speedups for motion
planning in autonomous vehicles~\cite{McNaughton_ICRA11}.
Such a rapid growth of general-purpose computing on GPUs,
\textit{a.k.a.}, GPGPU, is brought by recent advances in GPU
programming languages, such as CUDA~\cite{CUDA40}.

As these trends show, GPUs are becoming more and more useful for
general-purpose systems.
Nonetheless, system software support for GPUs in today's market is
tailored to accelerate particular applications dedicated to the
system; it is not well-designed to integrate GPUs into more
general-purpose multi-tasking systems.
Although the previous work raised above~\cite{Han_SIGCOMM10,
Jang_NSDI11, McNaughton_ICRA11} have drawn improvements in performance
achieved by GPUs, they could never provide performance or
quality-of-service (QoS) management without system software primitives.
Given that those networked and embedded systems are typically composed
of multiple clients and components, it is essential to manage GPUs as
first-class computing resources so that they can be accessed by multiple
tasks simultaneously in a reliable manner.

The research community has articulated the needs of enhancement in 
the operating system (OS)~\cite{Bautin_MCNC08, Kato_ATC11,
Rossbach_SOSP11}, hypervisor~\cite{Gupta_ATC11}, and runtime
library~\cite{Kato_RTSS11} to make GPUs available in interactive
and/or virtualized multi-tasking environments.
However, all these pieces of work depend highly on the user-space runtime
engine, often included as part of proprietary software, which provides
the user space with an application programming interface (API).
This framework limits the use of GPUs only within the user space.
For example, it prevents the file system or network stack in the OS from
using GPUs directly.
There is another issue of concern with this framework.
Since runtime support is employed in the user space, the device
driver needs to expose resource management primitives to the user space,
which implies that non-privileged user-space programs may abuse GPU
resources.
As a matter of fact, we can launch any program on an NVIDIA GPU without
using any user-space runtime libraries, but using an \texttt{ioctl}
system call directly. 
This explains that GPUs should be protected by the OS as well as CPUs.

In addition to those conceptual issues, there are more fundamental and
practical issues with publicly-available GPGPU software.
For example, memory allocation for GPU computing is not allowed to
exceed the physical capacity of device memory.
We are also not aware of any API that allows GPU contexts to share
memory resources with other contexts.
Such data management constraints may not be acceptable in
general-purpose programming.

We present \textbf{Gdev}, a new approach to GPU resource management that
addresses the current limitations of GPGPU ecosystems.
Gdev integrates runtime support for GPUs into the OS, which allows the
user space as well as the OS itself to use GPUs through the same API
set, and protects GPUs from non-privileged user-space programs at the OS
level.
Under this runtime-unified OS model, Gdev further provides first-class GPU
resource management primitives for multi-tasking systems.
More specifically, Gdev allows programmers to share device memory
resources among GPU contexts explicitly using the API. 
We also leverage this shared memory scheme to allow GPU contexts to
allocate memory beyond the physical size of device memory.
Finally, Gdev supports virtual GPUs to strongly isolate GPU resources
among working entities.
As a proof-of-concept, we provide an open-source implementation of Gdev.
In summary, this paper makes the following contributions:

\begin{itemize}
 \vspace{-0.25em}
 \item Identifies the advantage/disadvantage of integrating runtime
       support for GPUs into the OS.
 \vspace{-0.5em}
 \item Enables the OS itself to use GPUs.
 \vspace{-0.5em}
 \item Makes GPUs as first-class computing resources in multi-tasking
       systems -- memory management and scheduling for inter-process
       communication (IPC) and GPU virtualization.
 \vspace{-0.5em}
 \item Provides open-source implementations of the GPU device driver,
       runtime/API libraries, utility tools, and Gdev resource
       management components.
 \vspace{-0.5em}
 \item Demonstrates the capabilities of Gdev using real-world benchmarks
       and applications.
 \vspace{-0.25em}
\end{itemize}

\textbf{Organization:}
The rest of this paper is organized as follows.
Section~\ref{sec:model} provides the model and assumptions behind
this paper.
Section~\ref{sec:ecosystem} outlines the concept of Gdev.
Section~\ref{sec:memory_management} and \ref{sec:scheduling} present
Gdev memory management and scheduling schemes.
Section~\ref{sec:implementation} describes our prototype implementation,
and Section~\ref{sec:evaluation} demonstrates our detailed experimental
results.
Section~\ref{sec:related_work} discusses related work.
We provide our concluding remarks in Section~\ref{sec:conclusion}.
