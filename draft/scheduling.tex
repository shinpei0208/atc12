\section{GPU Scheduling}
\label{sec:scheduling}

The goal of the Gdev scheduler is to assign computation and data
transmission times for GPU contexts correctly based on the given
scheduling policy.
Although we make use of some previous
techniques~\cite{Kato_RTSS11,Kato_ATC11}, Gdev provides a new context
queuing scheme and virtual GPU support for general-purpose time-sharing
systems.

\subsection{Scheduling and Queuing}
\label{sec:scheduling_queueing}

Gdev uses a simiar scheme to TimeGraph~\cite{Kato_ATC11} for GPU
scheduling.
Specifically, it allows GPU contexts to use GPU resources only if no
other contexts are using the corresponding resources.
The pending GPU contexts are queued by the scheduler while waiting for
the current context holding the resources.
To notify the completion of the current context execution, Gdev
uses additional GPU commands to generate an interrupt from the GPU.
The highest-priority context is chosen from the queue upon every
interrupt, and dispatched to the GPU.
The computation and data transmission times are separately measured and
accumulated for resource accounting.
For compute requests, we also allow the same context to launch compute
instances simultaneously, and the total makespan from the first to the last
instance is deemed as the computation time.
PTask~\cite{Rossbach_SOSP11} and RGEM~\cite{Kato_RTSS11} also use
similar mechanisms, but do not use interrupts, and thereby resource
accounting is managed by the user space via the API.

The difference between Gdev and TimeGraph is that Gdev is API-driven,
invoking a scheduler only when \texttt{gmemcpy*} or \texttt{glaunch} is
called, while TimeGraph is command-driven, invoking a scheduler whenever
GPU commands are flushed.
In this regard, Gdev is similar to PTask~\cite{Rossbach_SOSP11} and
RGEM~\cite{Kato_RTSS11}.
However, Gdev differs even from these prior work in that it supports
separate queues for resource accounting of compute and memory-copy
operations, which we call \textit{Multiple Resource Queues} (MRQ), while
we call \textit{Single Device Queue} (SDQ) for the previous approach
where the scheduler supports only a single queue per device for resource
accounting.

The MRQ scheme is apparently more efficient than the SDQ scheme, when
different compute and memory-copy operations can be overlapped.
Suppose that there are two contexts both requesting 50\% of compute
and 50\% of memory-copy demands.
The SDQ scheme considers that the demand of each context is 100\% by
adding compute and memory-copy demands, and the total demand of the
two context is 200\%.
This workload thereby looks overloaded under the SDQ scheme.
The MRQ scheme, on the other hand, does not consider the total workload
to be overloaded while each resource to be fully utilized.

Gdev creates different scheduler threads for compute and memory-copy
operations. 
The compute scheduler thread is invoked upon the associated interrupt
generated by the GPU, while the memory-copy scheduler thread is awakened
by the Gdev runtime when the memory-copy operation is done, since we do
not use interrupts for memory-copy operations.

Priority assignments are focused on in this paper.
Gdev simply propagates the task priority, assigned by the OS, to the
corresponding GPU context. 

\subsection{Virtual GPU Support}
\label{sec:virtual_gpu}

Gdev is capable of partitining the time and space of the GPU to
virtualize a physical GPU to logical GPUs.
Users accessing different virtual GPUs are not interfered with each
other.
Virtual GPUs are activated by specifying the weights of GPU resources
assigned to each of them.
We classify GPU resources into the \textit{memory share}, \textit{memory
bandwidth}, and \textit{compute bandwidth}.
The memory share is the weight of the physical memory space available
for the virtual GPU, and Gdev simply reserves the physical memory space
in accordance with the specified memory share.
The memory bandwidth is the amount of time per some period given for
memory-copy operations related to the GPU, and the compute bandwidth is
that for GPU compute operations.
Gdev creates compute and memory-copy scheduler threads for each virtual
GPU.
The system comprising 4 virtual GPUs, for example, would contain 8
instances of the Gdev scheduler in total.
We however apply the same scheduling policy to the compute and
memory-copy schedulers.

The challenge for virtual GPU scheduling is raised by the non-preemptive
and burst nature of GPU workloads.
We have implemented the Credit scheduling algorithm supported by Xen
hypervisor~\cite{Barham_SOSP03} to verify if an existing virtual CPU
scheduling policy can be applied for a virtual GPU scheduler.
However, we have found that the Credit scheduler fails to maintain the
desired bandwidth for the virtual GPU, largely attributed to the fact
that it presumes preemptive constantly-working CPU workloads, while GPU
workloads are non-preemptive and bursting.

\begin{figure}[t]
 \begin{center}
  \begin{tabular}{l}
   \hline
   \hline
   {\small \verb|vgpu->bgt|: budget of the virtual GPU.}\\
   {\small \verb|vgpu->utl|: actual GPU utilization of the virtual GPU.}\\
   {\small \verb|vgpu->bw|: bandwidth assigned to the virtual
   GPU.}\\
   {\small \verb|current/next|: current/next virtual GPU selected for run.}\\
   \hline
   {\small \verb|void on_arrival(vgpu, ctx) {|}\\
   {\small \verb|  if (current && current != vgpu)|}\\
   {\small \verb|    suspend(ctx);|}\\
   {\small \verb|  dispatch(ctx);|}\\
   {\small \verb|}|}\\
   {\small \verb|vgpu_instace* on_completion(vgpu, ctx) {|}\\
   {\small \verb|  if (vgpu->bgt < 0 && vgpu->utl > vgpu->bw)|}\\
   {\small \verb|    move_to_queue_tail(vgpu);|}\\
   {\small \verb|  next = get_queue_head();|}\\
   {\small \verb|  if (!next) return null;|}\\
   {\small \verb|  if (next != vgpu && next->utl > next->bw) {|}\\
   {\small \verb|    wait_for_short();|}\\
   {\small \verb|    if (!current) return next;|}\\
   {\small \verb|    else return null;|}\\
   {\small \verb|  }|}\\
   {\small \verb|}|}\\
   \hline
  \end{tabular}
  \caption{Pseudo-code of the Band scheduler.}
  \label{fig:band}
 \end{center}
 \vspace{-2em}
\end{figure}

To overcome the virtual GPU scheduling problem, we propose a
\textit{bandwidth-aware non-preemptive device} (Band) scheduling
algorithm.
The pseudo-code of the Band scheduler is shown in
Figure~\ref{fig:band}.
The \texttt{on\_arrival} function is invoked by the task associated with
a context \texttt{ctx} assigned to a virtual GPU \texttt{vgpu} when
it is about to use GPU resources.
The context can be dispatched to the GPU only if no other virtual GPUs
are holding the GPU.
Otherwise, the corresponding task is suspended.
On the other hand, the \texttt{on\_completion} functions is called by
the corresponding scheduler thread upon the completion of a context
\texttt{ctx} assined to a virtual GPU \texttt{vgpu}, to select the next
virtual GPU to execute. 
Our scheduling policy is based on the Credit scheduler but differs in
the following two points.
First, the Band scheduler lowers the priority of the virtual GPU when
its budget (credit) is exhausted and its utilization of the GPU is
exceeding the assigned bandwidth, while the Credit scheduler always
lowers the priority when the budget is exhausted.
The second modification to the Credit scheduler is that the Band
scheduler waits for a certain amount of time specified by the system
designer, if the actual GPU utilization of the next virtual GPU selected
by the scheduler is exceeding its assigned bandwidth.
This particularly works for non-preemptive burst workloads.
Suppose that the system has two virtual GPUs.
Both the virtual GPUs execute burst workloads, but the lengh of each
non-preemptive execution differs.
If their workloads arrive in turn, they are also dispatched in turn, but
the GPU utilization could not be fair due to different lengths of
non-preemptive executions.
If the scheduler waits for a short time, however, the workload with
a short non-preemptive execution could arrive with the next request, and
can then be scheduled to meet the desired bandwidth.
If the workload does not arrive after the waiting, it implies that this
is not a burst workload, and hence is not emergent to meet the
bandwidth.
For non-burst workloads, the Band scheduler behaves as the Credit
scheduler in most cases.