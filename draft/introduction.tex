\section{Introduction}
\label{sec:introduction}

Recent innovations in heterogeneous compute devices with many-core
technology have continuously achieved orders-of-magnitude increases in
systems performance. 
In particular, the graphics processing unit (GPU) receives considerable
attention as a very powerful platform that embraces a concept of
many-core computing.
A recent announcement from the TOP500 supercomputing sites in November
2011~\cite{TOP500} disclosed that three of the top five supercomputers
use GPUs.
For example, scientific climate applications can gain 80x speedups using
such GPU-based supercomputers~\cite{Shimokawabe10}.
The benefits of GPUs appear not only in high-performance computing but
also in general-purpose and embedded computing domains.
According to previous research, GPUs can provide up to an order of 10x
speedups for software routers~\cite{Han_SIGCOMM10}, 20x speedups for
encrypted networks~\cite{Jang_NSDI11}, and 15x speedups for motion
planning in autonomous vehicles~\cite{McNaughton_ICRA11}.
Such a rapid growth of general-purpose computing on GPUs,
a.k.a., \textit{GPGPU}, is encouraged by recent advances in GPU
programming languages, such as CUDA~\cite{CUDA40}.
% and OpenCL~\cite{OpenCL12}.

Although the use of GPUs in general-purpose domains provides significant
performance improvements, system software support for GPUs in the market
is especially  tailored to accelerate particular applications dedicated
to the system, but is not well designed to integrate GPUs into more
general time-sharing systems.
Therefore, the research community has argued for the need to manage GPU
resources in the operating system (OS)~\cite{Bautin_MCNC08, Kato_ATC11,
Rossbach_SOSP11}.
However, these pieces of OS support still limit a class of applications
that can use GPUs due to a lack of first-class resource management
primitives, such as virtual memory management, inter-process
communication (IPC), and time-and-space partitioning.
On a time-sharing server where multiple users are logged in their own
sessions, for instance, if one executes a high-workload program using
the GPU, GPU 
resources available for other users could be very limited.
GPU schedulers in the state of the arts~\cite{Kato_ATC11,
Rossbach_SOSP11} support priority and reservation schemes, but users
are required to specify priorities or reserves by themselves, and thus
the system cannot partition GPU resources for independent sessions.
More essentially, the current system support does not allow
programmers to share device memory among multiple contexts, and the
total available data size is limited to the device memory size, which
would pose non-trivial constraints in general-purpose domains.
The current system support for GPUs also leaves the application
programming interface (API) to the user space, which restricts the
availability of GPUs to user-space applications.
In addition, employing the API in the user-space library implies that
the device driver exposes its resource management primitives to the user
space through a system call, which allows malicious programs to control
the GPU through this system call. 
As a matter of fact, non-privileged user-space programs can directly
allocate memory and launch computation on NVIDIA GPUs using
\texttt{ioctl} calls in Linux.
Hence, the API should also be protected by the OS.

This paper presents \textbf{Gdev}, a new GPU ecosystem that integrates
runtime support into the OS to enable a wide class of user-space
applications and the OS itself to use GPUs in a reliable manner.
Specifically, Gdev employs a set of low-level API functions for GPUs
in the OS.
While OS modules can directly call these functions, user-space
programs can also call them through a system call, such as POSIX
\texttt{ioctl()}.
We also provide CUDA 4.0 API~\cite{CUDA40}\footnote{We have
implemented a limited set of commonly-used CUDA 4.0 API functions but
not yet fully covered CUDA 4.0.}, wrapping Gdev API, so that legacy CUDA
applications can run on top of Gdev both in the user space and OS.
This runtime-unified OS approach forces all GPU applications running in
the same system to be managed by the identical resource management
entity, which eliminates the concern about malicious programs attempting
to access the GPU directly.
The contribution of Gdev also includes first-class support for
multi-tasking environments with GPUs.
Gdev provides shared memory support to allow programmers to
share device memory among multiple contexts explicitly through the API.
We also use this shared memory scheme to allow systems to allocate data
over the device memory size, exploiting implicit data eviction and
reloading between the host and device memory.
The final piece of our contribution is a new GPU scheduler, particularly
to support virtual GPUs under the non-preemptive and burst nature of GPU
workload.
As a proof-of-concept, we provide an open-source implementation of Gdev,
including a device driver, runtime library, and API library.

The rest of this paper is organized as follows.